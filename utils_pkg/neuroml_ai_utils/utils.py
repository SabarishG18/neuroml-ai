#!/usr/bin/env python3
"""
Misc utils

File: gen_rag/utils.py

Copyright 2025 Ankur Sinha
Author: Ankur Sinha <sanjay DOT ankur AT gmail DOT com>
"""

import logging
import os
import sys
import time

import httpx
import ollama
from langchain.chat_models import init_chat_model
from langchain.embeddings import init_embeddings
from langchain_core.messages import AIMessage
from langchain_core.output_parsers import JsonOutputParser
from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEndpointEmbeddings
from tenacity import (
    retry,
    retry_if_exception_type,
    stop_after_attempt,
    wait_random_exponential,
)


class LoggerNotInfoFilter(logging.Filter):
    """Allow only non INFO messages"""

    def filter(self, record):
        return record.levelno != logging.INFO


class LoggerInfoFilter(logging.Filter):
    """Allow only INFO messages"""

    def filter(self, record):
        return record.levelno == logging.INFO


logger_formatter_info = logging.Formatter(
    "%(name)s (%(levelname)s) >>> %(message)s\n\n"
)
logger_formatter_other = logging.Formatter(
    "%(name)s (%(levelname)s) in '%(funcName)s' >>> %(message)s\n\n"
)


def check_ollama_model(logger, model, exit=False):
    """Check if ollama model is available

    :param logger: logger instance
    :type logger: logging
    :param model: ollama model name
    :type model: str
    :param exit: if we should call sys.exit if check fails
    :type exit: bool
    :returns: None

    :throws ollama.ResponseError: if `model` is not available
    :throws ConnectionError: if cannot connect to an Ollama server

    """
    try:
        _ = ollama.show(model)
    except ollama.ResponseError:
        logger.error(f"Could not find ollama model: {model}")
        logger.error("Please ensure you have pulled the model")
        if exit:
            sys.exit(-1)
    except ConnectionError:
        logger.error("Could not connect to Ollama.")
        if exit:
            sys.exit(-1)


def parse_output_with_thought(message: AIMessage, schema) -> dict:
    """Parse AI message with thought to a dict based on given schema"""
    if "</think>" in message.content:
        splits = message.content.split("</think>")
        answer = splits[1].strip()
    else:
        answer = message.content

    parser = JsonOutputParser()
    parser.pydantic_object = schema()
    result = parser.parse(answer)
    return result


def split_thought_and_output(message: AIMessage):
    """Split out thoughts and actual responses from AI responses"""
    if "</think>" in message.content:
        splits = message.content.split("</think>")
        answer = splits[1].strip()
        thoughts = splits[1].strip()
    else:
        answer = message.content.strip()
        thoughts = ""
    return thoughts, answer


def check_model_works(model, timeout=30, retries=3):
    """Check if a model works since it is not tested when loaded"""
    for attempt in range(retries):
        try:
            # Use a very simple prompt with short max_tokens
            _ = model.invoke("test", config={"timeout": timeout})
            return True, f"Model available (attempt {attempt + 1})"
        except Exception as e:
            error_msg = str(e)
            if attempt < retries - 1:
                time.sleep(2**attempt)  # Exponential backoff
            else:
                return False, f"Model unavailable after {retries} attempts: {error_msg}"
    return False, "Unknown error"


def setup_embedding(model_name_full, logger):
    # need to use inference providers
    if model_name_full.lower().startswith("huggingface:"):
        _, model_name, provider = model_name_full.split(":")
        logger.debug(f"Using huggingface model: {model_name}")

        hf_token = os.environ.get("HF_TOKEN", None)
        assert hf_token

        model_var = HuggingFaceEndpointEmbeddings(
            model=f"{model_name}",
            provider="auto",
            task="feature-extraction",
            huggingfacehub_api_token=hf_token,
        )
    else:
        if model_name_full.lower().startswith("ollama:"):
            check_ollama_model(logger, model_name_full.lower().replace("ollama:", ""))
        model_var = init_embeddings(model_name_full)

    assert model_var
    logger.info(f"Using embedding model: {model_name_full}")
    return model_var


def setup_llm(model_name_full, logger):
    """Set up a chat model"""
    if model_name_full.lower().startswith("huggingface:"):
        _, model_name, provider = model_name_full.split(":")
        logger.debug(f"Using huggingface model: {model_name}")

        hf_token = os.environ.get("HF_TOKEN", None)
        assert hf_token

        llm = HuggingFaceEndpoint(
            repo_id=f"{model_name}",
            provider="auto",
            max_new_tokens=512,
            do_sample=False,
            repetition_penalty=1.03,
            huggingfacehub_api_token=hf_token,
        )

        model_var = init_chat_model(
            model_name,
            model_provider="huggingface",
            llm=llm,
            configurable_fields=("temperature"),
        )
    else:
        if model_name_full.lower().startswith("ollama:"):
            check_ollama_model(logger, model_name_full.lower().replace("ollama:", ""))

        model_var = init_chat_model(
            model_name_full, configurable_fields=("temperature")
        )
    assert model_var

    state, msg = check_model_works(model_var)
    assert state
    logger.info(f"Using chat model: {model_name_full}")

    return model_var


@retry(
    wait=wait_random_exponential(multiplier=1, max=10),
    stop=stop_after_attempt(10),
    retry=retry_if_exception_type(
        (httpx.ConnectError, httpx.HTTPStatusError, httpx.ReadError, httpx.ReadTimeout)
    ),
    reraise=True,
)
async def check_api_is_ready(url: str):
    """Exponentially drop off checking that API is ready

    :param url: url of health end point
    :type url: str

    """
    async with httpx.AsyncClient() as client:
        response = await client.get(url)
        response.raise_for_status()

        return response.json()
